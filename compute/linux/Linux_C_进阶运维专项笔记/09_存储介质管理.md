# 09_存储介质管理

[TOC]

sky92@sky92:~$

sky92@sky92-vmwarevirtualplatform:~$

## Raid技术

### 介绍

raid = Redundant Array of Independent Driver  冗余磁盘阵列

```shell
#raid0：至少2块磁盘，每份数据拆分写入多块物理盘，理论速度=单盘速*盘数，任一物理盘损坏则数据全失，速度快，性能高，安全低，空间利用100%；

#raid1：至少2块磁盘，每份数据分别写入多块物理盘，读写时各盘数据同步，即实时备份，安全高，速度=单盘速度，可用空间=最低单盘容量，利用率低；用于存放重要资料；

#raid2：至少3块磁盘，读写时校验，同时编码后写入，速度慢，已淘汰；

#raid3：至少3块磁盘，带有异或校验的模式，AxorBxorC，多值异或计算，判断奇偶，用于校验反推，一般同时只允许损坏一块数据盘或校验盘；
#disk1：0101
#disk2：1011
#异或值：1110
#disk1坏，但知道disk2的值，同时直到异或值得，可反推disk1数据；
#使用disk3存储异或值，称为校验盘，disk1 disk2 为数据盘 
#若损坏则无法恢复数据，不过换新disk3后可以重新写入 数据盘计算的异或；数据盘正常时的校验盘损坏还是可以补救的；
#所以空间利用率看情况，校验盘容量要大于数据盘中最大的单盘容量，可用容量即数据盘总容量；

#raid4：至少3块磁盘，raid3的变种，相当于以数据块分割数据的raid3，很少用；

#raid5：至少3块磁盘，升级版raid3，其异或校验码存放在每块磁盘内，每块都是数据盘且是其他几个盘的校验盘，不能同时损坏2块；多块硬盘做冗余的时候会牺牲相当于1块硬盘空间做校验；可用磁盘空间=(N-1)*单盘容量
#raid5的损坏恢复中的URE错误会导致数据恢复失败，重建成功率低，所以出现了raid6
#raid5只适合盘少且数据不是那么重要的用户，以前民用较多，其实就低价硬盘和多盘位的普及，该模式已经要被raid6取代了；

#raid F1： raid5针对SSD的变种；

#raid6：至少4块磁盘，raid5升级版，牺牲相当于2块硬盘空间做校验，保存了两份不同算法的校验码，计算量大，速度比raid5慢

#raid10：至少4块磁盘，raid1 和 raid0的组合 ，也叫混合raid，即4块盘两两组为raid1，然后再次组合为raid0，兼顾性能和安全，但是空间利用率只有50%；

#raid50：至少6块磁盘，参照raid10

#raid60：至少8块磁盘，参照raid10

#JBOD：常规顺序储存，空间利用率100%，没有校验和冗余，坏盘只影响部分数据，类似LLVM和win的跨区卷；

#unraid：带有冗余校验的JBOD，可以指定校验盘；校验盘容量要大于数据盘最大单盘容量；

#raid-z：用于ZFS系统的软raid，ZFS为128位的文件系统，不需要任何额外软件或硬件就可以处理raid，radi-z为ZFS的自身特性，无需其他软件；
#raidz1：2盘数据  1盘校验
#raidz2：2盘数据  2盘校验
#raidz3：2盘数据  3盘校验
#raidz非常占内存，每TB数据需要配1G内存 ，建议配合ECC纠错内存 
#raidz扩容麻烦，扩容需要新加一组
```

### 异或

```python
#异或运算演示 相同为1 不同为0
#多值异或计算1的个数     奇数个为1 偶数个为0
sky92@sky92:~$ python
Python 2.7.12 (default, Mar  1 2021, 11:38:31)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> 8^8
0 
>>> 8^9
1
>>> 1^0
1
>>> 0^0
0 
>>> 1^0^0
1
>>> 1^1^0
0
>>> 1^1^1
1


```

### 搭建raid

```shell
#搭建raid10，就是raid1 加上raid0 ，需要四块磁盘，这里我们使用vmware虚拟机进行搭建
sky92@sky92-vmwarevirtualplatform:~$ ll /dev/sd*
brw-rw---- 1 root disk 8,  0 10月  7 15:33 /dev/sda
brw-rw---- 1 root disk 8,  1 10月  7 15:33 /dev/sda1
brw-rw---- 1 root disk 8, 16 10月  7 15:33 /dev/sdb
brw-rw---- 1 root disk 8, 32 10月  7 15:33 /dev/sdc
brw-rw---- 1 root disk 8, 48 10月  7 15:33 /dev/sdd
brw-rw---- 1 root disk 8, 64 10月  7 15:33 /dev/sde

##----------------------------------------------------------------------##
#可见bcde四块盘
sky92@sky92-vmwarevirtualplatform:~$ sudo fdisk -l |grep sd 
Disk /dev/sda: 30 GiB, 32212254720 bytes, 62914560 sectors
/dev/sda1        2048 62910539 62908492  30G 83 Linux
Disk /dev/sdc: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sdd: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sdb: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sde: 2 GiB, 2147483648 bytes, 4194304 sectors

##----------------------------------------------------------------------##
#mdadm是用于建设管理加监控RAID的命令工具
#安装mdadm软件 
sky92@sky92-vmwarevirtualplatform:~$ sudo apt-get install mdadm
正在读取软件包列表... 完成
正在分析软件包的依赖关系树... 完成
正在读取状态信息... 完成
下列软件包是自动安装的并且现在不需要了：
  libffi7
使用'sudo apt autoremove'来卸载它(它们)。
将会同时安装下列软件：
  finalrd
建议安装：
  default-mta | mail-transport-agent dracut-core
下列【新】软件包将被安装：
  finalrd mdadm
升级了 0 个软件包，新安装了 2 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。
需要下载 471 kB 的归档。
解压缩后会消耗 1,239 kB 的额外空间。
您希望继续执行吗？ [Y/n] y
获取:1 http://mirrors.aliyun.com/ubuntu jammy/main amd64 finalrd all 9build1 [7,306 B]
获取:2 http://mirrors.aliyun.com/ubuntu jammy/main amd64 mdadm amd64 4.2-0ubuntu1 [464 kB]
已下载 471 kB，耗时 5秒 (99.6 kB/s)
正在预设定软件包 ...
正在选中未选择的软件包 finalrd。
(正在读取数据库 ... 系统当前共安装有 296477 个文件和目录。)
准备解压 .../finalrd_9build1_all.deb  ...
正在解压 finalrd (9build1) ...
正在选中未选择的软件包 mdadm。
准备解压 .../mdadm_4.2-0ubuntu1_amd64.deb  ...
正在解压 mdadm (4.2-0ubuntu1) ...
正在设置 finalrd (9build1) ...
Created symlink /etc/systemd/system/sysinit.target.wants/finalrd.service → /lib/systemd/system/finalrd.service.
正在设置 mdadm (4.2-0ubuntu1) ...
Generating mdadm.conf... done.
update-initramfs: deferring update (trigger activated)
Sourcing file `/etc/default/grub'
Sourcing file `/etc/default/grub.d/init-select.cfg'
Sourcing file `/etc/default/grub.d/lubuntu-grub-theme.cfg'
Generating grub configuration file ...
Found theme: /usr/share/grub/themes/lubuntu-grub-theme/theme.txt
Found linux image: /boot/vmlinuz-5.15.0-47-generic
Found initrd image: /boot/initrd.img-5.15.0-47-generic
Found linux image: /boot/vmlinuz-5.15.0-46-generic
Found initrd image: /boot/initrd.img-5.15.0-46-generic
Found memtest86+ image: /boot/memtest86+.elf
Found memtest86+ image: /boot/memtest86+.bin
Warning: os-prober will not be executed to detect other bootable partitions.
Systems on them will not be added to the GRUB boot configuration.
Check GRUB_DISABLE_OS_PROBER documentation entry.
done
Created symlink /etc/systemd/system/mdmonitor.service.wants/mdcheck_continue.timer → /lib/systemd/system/mdcheck_continue.timer.
Created symlink /etc/systemd/system/mdmonitor.service.wants/mdcheck_start.timer → /lib/systemd/system/mdcheck_start.timer.
Created symlink /etc/systemd/system/mdmonitor.service.wants/mdmonitor-oneshot.timer → /lib/systemd/system/mdmonitor-oneshot.timer.
正在处理用于 man-db (2.10.2-1) 的触发器 ...
正在处理用于 initramfs-tools (0.140ubuntu13) 的触发器 ...
update-initramfs: Generating /boot/initrd.img-5.15.0-47-generic

##----------------------------------------------------------------------##
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm --help
mdadm is used for building, managing, and monitoring
Linux md devices (aka RAID arrays)
Usage: mdadm --create device options...
            Create a new array from unused devices.
       mdadm --assemble device options...
            Assemble a previously created array.
       mdadm --build device options...
            Create or assemble an array without metadata.
       mdadm --manage device options...
            make changes to an existing array.
       mdadm --misc options... devices
            report on or modify various md related devices.
       mdadm --grow options device
            resize/reshape an active array
       mdadm --incremental device
            add/remove a device to/from an array as appropriate
       mdadm --monitor options...
            Monitor one or more array for significant changes.
       mdadm device options...
            Shorthand for --manage.
Any parameter that does not start with '-' is treated as a device name
or, for --examine-bitmap, a file name.
The first such name is often the name of an md device.  Subsequent
names are often names of component devices.

 For detailed help on the above major modes use --help after the mode
 e.g.
         mdadm --assemble --help
 For general help on options use
         mdadm --help-options
##----------------------------------------------------------------------##
#创建raid10
sky92@sky92-vmwarevirtualplatform:~$ mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sdd /dev/sde
mdadm: must be super-user to perform this action
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -Cv /dev/md0 -a yes -n 4 -l 10 /dev/sdb /dev/sdc /dev/sdd /dev/sde
mdadm: layout defaults to n2
mdadm: layout defaults to n2
mdadm: chunk size defaults to 512K
mdadm: size set to 2094080K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
#此时得到了 /dev/md0 阵列设备

# -C表示创建阵列
# -v表示显示创建过程
# /dev/md0 raid阵列的名字
# -a yes 表示自动创建阵列设备文件
# -n 4 表示用4块盘创建
# -l 10 表示raid的级别
# 最后加上四块磁盘设备
# -x 指定使用几块硬盘做RAID的热备用盘，x1表示保留一块空闲的硬盘作备用

##----------------------------------------------------------------------##
#格式化阵列
sky92@sky92-vmwarevirtualplatform:~$ sudo mkfs.xfs /dev/m
mapper/ mcelog  md0     mem     midi    mqueue/
sky92@sky92-vmwarevirtualplatform:~$ sudo mkfs.xfs /dev/md0
log stripe unit (524288 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/md0               isize=512    agcount=8, agsize=130944 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=4096   blocks=1047040, imaxpct=25
         =                       sunit=128    swidth=256 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

##----------------------------------------------------------------------##
# 针对分区做文件夹挂载，开始使用  
# mount 设备名 挂载点
sky92@sky92-vmwarevirtualplatform:/mnt$ sudo mkdir /mnt/raid10
sky92@sky92-vmwarevirtualplatform:/mnt$ sudo mount /dev/md0 /mnt/raid10/ 
sky92@sky92-vmwarevirtualplatform:/mnt$ sudo mount -l | grep md0
/dev/md0 on /mnt/raid10 type xfs (rw,relatime,attr2,inode64,logbufs=8,logbsize=32k,sunit=1024,swidth=2048,noquota)

# df 查看容量情况
sky92@sky92-vmwarevirtualplatform:/mnt$ sudo df -hT |grep md0
/dev/md0       xfs    4.0G   61M  4.0G   2% /mnt/raid10

#检查raid10的详细信息
sky92@sky92-vmwarevirtualplatform:/mnt$ sudo mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Oct  7 15:55:30 2022
        Raid Level : raid10
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 16:01:27 2022
             State : clean
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 12553929:1b4fd1c9:b0d72456:e37441a3
            Events : 17

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       2       8       48        2      active sync set-A   /dev/sdd
       3       8       64        3      active sync set-B   /dev/sde
       
##----------------------------------------------------------------------##
#测试
root@sky92-vmwarevirtualplatform:/mnt/raid10# echo {1..9000000} >> raid_test.txt
root@sky92-vmwarevirtualplatform:/mnt/raid10# echo {1..9000000} >> raid_test.txt
root@sky92-vmwarevirtualplatform:/mnt/raid10# exit
exit
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo du -h *
252M    raid_test.txt
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo df -hT | grep md0
/dev/md0       xfs    4.0G  314M  3.7G   8% /mnt/raid10

sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo cp raid_test.txt raid_test1.txt
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo cp raid_test.txt raid_test2.txt
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo df -hT | grep md0
/dev/md0       xfs    4.0G  584M  3.5G  15% /mnt/raid10
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo du -h *
136M    raid_test1.txt
136M    raid_test2.txt
252M    raid_test.txt
##----------------------------------------------------------------------##
#卸载阵列
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ sudo umount /dev/md0
umount: /mnt/raid10: target is busy.
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ cd /
sky92@sky92-vmwarevirtualplatform:/$ sudo umount /dev/md0

##----------------------------------------------------------------------##
#开机挂载方法 持久化挂载 编辑/etc/fstab文件
sky92@sky92-vmwarevirtualplatform:/mnt/raid10$ cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a device; this may
# be used with UUID= as a more robust way to name devices that works even if
# disks are added and removed. See fstab(5).
# 文件系统					 文件挂载目录	    格式     挂载方式	   dump备份 开机校验
# <file system>             <mount point>  <type>  <options>  <dump>  <pass>
UUID=a55d6f54-f1f1-415e-baec-d5e5768b81c6 /              ext4    defaults   0 1 
 
#在文件/etc/fstab 添加如下 该步骤慎用 由于重启开机会导致设备名称/dev/md0 变为/dev/md127等其他名称而无法正常开机 所以推荐使用UUID挂载  
/dev/md0                   /mnt/raid10     xfs     defaults   0       0		

# 注意这里只有一个md127  推荐使用UUID挂载
sky92@sky92-vmwarevirtualplatform:~$ ll /dev/md*
brw-rw---- 1 root disk 9, 127 10月  7 16:31 /dev/md127
/dev/md:
total 0
drwxr-xr-x  2 root root   60 10月  7 16:31 ./
drwxr-xr-x 20 root root 4260 10月  7 16:31 ../
lrwxrwxrwx  1 root root    8 10月  7 16:31 sky92-vmwarevirtualplatform:0 -> ../md127
#查询UUID
sky92@sky92-vmwarevirtualplatform:~$ blkid
/dev/sdd: UUID="12553929-1b4f-d1c9-b0d7-2456e37441a3" UUID_SUB="01087303-55f6-aad0-9b72-7328dbd4dcad" LABEL="sky92-vmwarevirtualplatform:0" TYPE="l
/dev/md127: UUID="529b3360-86ee-48a7-9dcf-dbfdce5b4e10" BLOCK_SIZE="512" TYPE="xfs"
/dev/sdb: UUID="12553929-1b4f-d1c9-b0d7-2456e37441a3" UUID_SUB="a2ecb481-4b67-b00e-1c90-db1b222cb622" LABEL="sky92-vmwarevirtualplatform:0" TYPE="l
/dev/sde: UUID="12553929-1b4f-d1c9-b0d7-2456e37441a3" UUID_SUB="eace4d50-9e25-0b3a-210a-37db1f2a045b" LABEL="sky92-vmwarevirtualplatform:0" TYPE="l
/dev/sdc: UUID="12553929-1b4f-d1c9-b0d7-2456e37441a3" UUID_SUB="163dc8ce-caeb-57f7-954b-ddcf87c77b8e" LABEL="sky92-vmwarevirtualplatform:0" TYPE="l
/dev/sda1: UUID="a55d6f54-f1f1-415e-baec-d5e5768b81c6" BLOCK_SIZE="4096" TYPE="ext4" PARTUUID="bffa1f4c-01"
##
sky92@sky92-vmwarevirtualplatform:~$ lsblk -f
NAME    FSTYPE       FSVER LABEL                         UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
loop0   squashfs     4.0                                                                            0   100% /snap/bare/5
loop1   squashfs     4.0                                                                            0   100% /snap/core20/1623
loop2   squashfs     4.0                                                                            0   100% /snap/firefox/1860
loop3   squashfs     4.0                                                                            0   100% /snap/gnome-3-38-2004/115
loop4   squashfs     4.0                                                                            0   100% /snap/firefox/1918
loop5   squashfs     4.0                                                                            0   100% /snap/hunspell-dictionaries-1-7-2004/2
loop6   squashfs     4.0                                                                            0   100% /snap/gnome-3-38-2004/119
loop7   squashfs     4.0                                                                            0   100% /snap/snapd/16778
loop8   squashfs     4.0                                                                            0   100% /snap/gtk-common-themes/1535
loop9   squashfs     4.0                                                                            0   100% /snap/snapd/17029
sda
└─sda1  ext4         1.0                                 a55d6f54-f1f1-415e-baec-d5e5768b81c6     20G    27% /var/snap/firefox/common/host-hunspell
                                                                                                             /
sdb     linux_raid_m 1.2   sky92-vmwarevirtualplatform:0 12553929-1b4f-d1c9-b0d7-2456e37441a3
└─md127 xfs                                              529b3360-86ee-48a7-9dcf-dbfdce5b4e10    3.5G    11% /mnt/raid10
sdc     linux_raid_m 1.2   sky92-vmwarevirtualplatform:0 12553929-1b4f-d1c9-b0d7-2456e37441a3
└─md127 xfs                                              529b3360-86ee-48a7-9dcf-dbfdce5b4e10    3.5G    11% /mnt/raid10
sdd     linux_raid_m 1.2   sky92-vmwarevirtualplatform:0 12553929-1b4f-d1c9-b0d7-2456e37441a3
└─md127 xfs                                              529b3360-86ee-48a7-9dcf-dbfdce5b4e10    3.5G    11% /mnt/raid10
sde     linux_raid_m 1.2   sky92-vmwarevirtualplatform:0 12553929-1b4f-d1c9-b0d7-2456e37441a3
└─md127 xfs                                              529b3360-86ee-48a7-9dcf-dbfdce5b4e10    3.5G    11% /mnt/raid10
sr0
##
sky92@sky92-vmwarevirtualplatform:~$ ll /dev/disk/by-uuid
total 0
drwxr-xr-x 2 root root  80 10月  7 16:31 ./
drwxr-xr-x 6 root root 120 10月  7 16:31 ../
lrwxrwxrwx 1 root root  11 10月  7 16:31 529b3360-86ee-48a7-9dcf-dbfdce5b4e10 -> ../../md127
lrwxrwxrwx 1 root root  10 10月  7 16:31 a55d6f54-f1f1-415e-baec-d5e5768b81c6 -> ../../sda1


#可见阵列的UUID为 529b3360-86ee-48a7-9dcf-dbfdce5b4e10  且名称变为了md127
#那么 应该填入如下
UUID=529b3360-86ee-48a7-9dcf-dbfdce5b4e10  /mnt/raid10 xfs defaults 0  0		


##----------------------------------------------------------------------##
#故障恢复  
sky92@sky92-vmwarevirtualplatform:~$ sudo  fdisk -l | grep sd
Disk /dev/sda: 30 GiB, 32212254720 bytes, 62914560 sectors
/dev/sda1        2048 62910539 62908492  30G 83 Linux
Disk /dev/sdb: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sdc: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sdd: 2 GiB, 2147483648 bytes, 4194304 sectors
Disk /dev/sde: 2 GiB, 2147483648 bytes, 4194304 sectors
#首先使用删除命令模拟掉盘故障模拟一块掉盘
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm /dev/md127 -f /dev/sdd
mdadm: set /dev/sdd faulty in /dev/md127
#可以看到这里有一块坏盘Failed Devices
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md127
/dev/md127:
           Version : 1.2
     Creation Time : Fri Oct  7 15:55:30 2022
        Raid Level : raid10
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 16:46:57 2022
             State : clean, degraded
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 12553929:1b4fd1c9:b0d72456:e37441a3
            Events : 21

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       -       0        0        2      removed
       3       8       64        3      active sync set-B   /dev/sde

       2       8       48        -      faulty   /dev/sdd

#可见仅仅损坏一块盘 raid10 一样可以运行
#若需要替换坏盘则要先取消挂载 然后重启
sky92@sky92-vmwarevirtualplatform:~$ sudo umount /dev/md127
sky92@sky92-vmwarevirtualplatform:~$ sudo reboot
#若已经挂载 则线取消挂载 再次加入好的磁盘  等待修复
sky92@sky92-vmwarevirtualplatform:~$ sudo umount /dev/md127 
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm /dev/md127 -a /dev/sdd
mdadm: added /dev/sdd 
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md127
/dev/md127:
           Version : 1.2
     Creation Time : Fri Oct  7 15:55:30 2022
        Raid Level : raid10
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 16:57:03 2022
             State : clean, degraded, recovering
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

    Rebuild Status : 85% complete

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 12553929:1b4fd1c9:b0d72456:e37441a3
            Events : 68

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       4       8       48        2      spare rebuilding   /dev/sdd
       3       8       64        3      active sync set-B   /dev/sde

#可见添加新盘后这里有个重建   Rebuild Status : 85% complete
##----------------------------------------------------------------------##  
#raid10重启
#生成配置文件
sky92@sky92-vmwarevirtualplatform:~$ sudo echo DEVICE /dev/sd[b-e] > /etc/mdadm/mdadm.conf
sky92@sky92-vmwarevirtualplatform:~$ cat /etc/mdadm/mdadm.conf
DEVICE /dev/sdb /dev/sdc /dev/sdd /dev/sde
#扫描
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -Ds >> /etc/mdadm/mdadm.conf
sky92@sky92-vmwarevirtualplatform:~$ sudo cat /etc/mdadm/mdadm.conf
DEVICE /dev/sdb /dev/sdc /dev/sdd /dev/sde
ARRAY /dev/md/sky92-vmwarevirtualplatform:0 metadata=1.2 name=sky92-vmwarevirtualplatform:0 UUID=12553929:1b4fd1c9:b0d72456:e37441a3
#若已经挂载 取消raid10挂载
sky92@sky92-vmwarevirtualplatform:~$ sudo umount /mnt/raid10/
#停止磁盘整列组
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -S /dev/md127
mdadm: stopped /dev/md127
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md127
mdadm: cannot open /dev/md127: No such file or directory
 
#然后就要使用配置文件正常启动raid10  
sky92@sky92-vmwarevirtualplatform:~$ sudo cat mdadm.conf
DEVICE /dev/sdb /dev/sdc /dev/sdd /dev/sde
ARRAY /dev/md/sky92-vmwarevirtualplatform:0 metadata=1.2 name=sky92-vmwarevirtualplatform:0 UUID=12553929:1b4fd1c9:b0d72456:e37441a3
#这里需要把配置文件内全称加上
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -A /dev/md/sky92-vmwarevirtualplatform:0
mdadm: /dev/md/sky92-vmwarevirtualplatform:0 has been started with 4 drives.
#然后就能看信息了 可见名称回到了 0
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Oct  7 15:55:30 2022
        Raid Level : raid10
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 4
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 17:04:06 2022
             State : clean
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 0

            Layout : near=2
        Chunk Size : 512K

Consistency Policy : resync

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 12553929:1b4fd1c9:b0d72456:e37441a3
            Events : 72

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync set-A   /dev/sdb
       1       8       32        1      active sync set-B   /dev/sdc
       4       8       48        2      active sync set-A   /dev/sdd
       3       8       64        3      active sync set-B   /dev/sde

#同样文件也没有损坏
sky92@sky92-vmwarevirtualplatform:~$ ll /mnt/raid10/
total 415372
drwxrwxrwx 2 root root        71 10月  7 16:11 ./
drwxr-xr-x 3 root root      4096 10月  7 16:01 ../
-rwxr-xr-x 1 root root 141777792 10月  7 16:11 raid_test1.txt*
-rwxr-xr-x 1 root root 141777792 10月  7 16:11 raid_test2.txt*
-rwxrwxrwx 1 root root 141777792 10月  7 16:09 raid_test.txt*


##----------------------------------------------------------------------## 
#删除raid

#卸载
sky92@sky92-vmwarevirtualplatform:~$ sudo umount /dev/md0

#停止raid
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -S /dev/md0
mdadm: stopped /dev/md0 

#消除磁盘信息
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm --misc --zero-superblock /dev/sdb
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm --misc --zero-superblock /dev/sdc
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm --misc --zero-superblock /dev/sdd
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm --misc --zero-superblock /dev/sde

#删除配置文件
sky92@sky92-vmwarevirtualplatform:~$ sudo rm /etc/mdadm/mdadm.conf

#消除开机自动挂载的配置 
sky92@sky92-vmwarevirtualplatform:~$ sudo nano  /etc/fstab
sky92@sky92-vmwarevirtualplatform:~$ cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a device; this may
# be used with UUID= as a more robust way to name devices that works even if
# disks are added and removed. See fstab(5).
#
# <file system>             <mount point>  <type>  <options>  <dump>  <pass>
UUID=a55d6f54-f1f1-415e-baec-d5e5768b81c6 /              ext4    defaults   0 1
#UUID=529b3360-86ee-48a7-9dcf-dbfdce5b4e10 /mnt/raid10    xfs     defaults   0 0


##----------------------------------------------------------------------## 
#raid和热备份盘
#先组成raid5 然后第四盘作为热备盘
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -Cv /dev/md0 -n 3 -l 5  /dev/sdb /dev/sdc /dev/sdd -x 1 /dev/sde
mdadm: layout defaults to left-symmetric
mdadm: layout defaults to left-symmetric
mdadm: chunk size defaults to 512K
mdadm: size set to 2094080K
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.

#检查 可见  sde为热备盘
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Oct  7 17:21:29 2022
        Raid Level : raid5
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 17:21:40 2022
             State : clean
    Active Devices : 3
   Working Devices : 4
    Failed Devices : 0
     Spare Devices : 1

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 49e37c59:ab01cb5c:2c9ab3d2:38230ce6
            Events : 18

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
       4       8       48        2      active sync   /dev/sdd

       3       8       64        -      spare   /dev/sde

#格式化
sky92@sky92-vmwarevirtualplatform:~$ sudo mkfs.xfs -f /dev/md0
log stripe unit (524288 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/md0               isize=512    agcount=8, agsize=130944 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=4096   blocks=1047040, imaxpct=25
         =                       sunit=128    swidth=256 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
#挂载写入数据检查
sky92@sky92-vmwarevirtualplatform:~$ sudo mkdir /mnt/raid5
sky92@sky92-vmwarevirtualplatform:~$ sudo mount /dev/md0 /mnt/raid5
sky92@sky92-vmwarevirtualplatform:~$ sudo echo {1..1000000} >> /mnt/raid5/raidtest.txt 
sky92@sky92-vmwarevirtualplatform:~$ sudo df -hT
Filesystem     Type   Size  Used Avail Use% Mounted on
tmpfs          tmpfs  389M  1.6M  388M   1% /run
/dev/sda1      ext4    30G  7.9G   21G  29% /
tmpfs          tmpfs  1.9G     0  1.9G   0% /dev/shm
tmpfs          tmpfs  5.0M  4.0K  5.0M   1% /run/lock
tmpfs          tmpfs  389M   68K  389M   1% /run/user/119
tmpfs          tmpfs  389M   68K  389M   1% /run/user/1000
/dev/md0       xfs    4.0G   68M  4.0G   2% /mnt/raid5

#开始模拟掉盘  
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm /dev/md0 -f /dev/sdc
mdadm: set /dev/sdc faulty in /dev/md0
#查看细节 热备盘sde自动加入了阵列
sky92@sky92-vmwarevirtualplatform:~$ sudo mdadm -D /dev/md0
/dev/md0:
           Version : 1.2
     Creation Time : Fri Oct  7 17:21:29 2022
        Raid Level : raid5
        Array Size : 4188160 (3.99 GiB 4.29 GB)
     Used Dev Size : 2094080 (2045.00 MiB 2144.34 MB)
      Raid Devices : 3
     Total Devices : 4
       Persistence : Superblock is persistent

       Update Time : Fri Oct  7 17:27:59 2022
             State : clean
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 1
     Spare Devices : 0

            Layout : left-symmetric
        Chunk Size : 512K

Consistency Policy : resync
	Rebuild Status : 86% complete

              Name : sky92-vmwarevirtualplatform:0  (local to host sky92-vmwarevirtualplatform)
              UUID : 49e37c59:ab01cb5c:2c9ab3d2:38230ce6
            Events : 37

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       3       8       64        1      active sync   /dev/sde
       4       8       48        2      active sync   /dev/sdd

       1       8       32        -      faulty   /dev/sdc

 
```

## LVM卷管理

### 介绍

raid阵列缺点在于创建时的大小已经决定，扩容比较麻烦，raid不同磁盘分区相对独立 可能空间利用率很低，分区满了之后则无法自动扩容，只能手动扩容；那么存储服务要求高的时候，可能就不适合单独使用raid；于是有LVM技术；

LVM= logical volume manager 逻辑卷管理技术，多个盘在逻辑上当作一个盘进行使用，动态磁盘容量管理，LVM灵活性较大；把物理磁盘所有的空间抽象为一个个约为2M的小块（逻辑区域1M或2M都可），然后以小块的集合作为存储池（逻辑卷组）；逻辑卷组决定了可分配的总容量，可用容量从逻辑卷组分配得到，被分配的可以作为日常存储空间，如用可用容量满了就再次向逻辑卷组的未分配空间申请空间即可，若物理磁盘也满了或想要增加物理空间，就新增硬盘，然后把新硬盘空间也抽象为很多2M小块加入逻辑卷组，增加逻辑卷组的未分配空间，则可以继续做后续存储；



LVM的两种主要方式：

基于硬盘创建LVM：多块硬盘做成逻辑卷，对逻辑卷统一管理，对分区动态扩容；

基于分区创建LVM：硬盘的多个分区由LVM统一管理为卷组，可弹性调整大小充分利用磁盘空间，文件系统建立在逻辑卷上；



常见名词（层层抽象）：

​	PP=physical parttion 物理分区 LVM直接创建在物理分区上；

​	PV=physical volume 物理卷，处于LVM的底层，一个PV对应一个PP，相当于物理分区的抽象；

​	PE=physical extends 物理区域单元，PV中可用于分配的最小单位，常为1M 或 2M ， PV会被分割为PE；

​	VG=volume group 卷组 创建在PV上，可划分多个物理卷，统一管理所有PE集合；

​	LE=logical extens 逻辑扩展单元 LE是组成LV的基本单元 一个LE对应了一个PE；

​	LV=logical volume 逻辑卷，创建在VG上，可动态扩容的概念，其实就是LE集合组成的空间；



增减空间容量就是控制PE的集合大小；

PE一般认为是4M，LVM最多创建65534个PE，所以LVM最大的VG是256G，但是以前LVM1由于linux内核限制每个逻辑卷（LV）有65,536个物理盘区（PE）的限制，但是从LVM2开始，没有限制PE的数量，我们演示使用的是LVM2；逻辑卷可理解为未分配的分区（/dev/sdc）；



### 流程解释

```shell
`
#物理分区 fdisk或partd格式化硬盘为8e（LVM文件格式）类型；如果是针对物理盘创建则无需这步；
#PV创建，pvcreate pvdisplay pvs将linux分区改为物理卷PV 并查看
#创建VG，通过vgcreate vgdisplay vgs 将物理卷改为物理卷组 并查看
#创建LV，通过lvcreate lvs将卷组分为若干逻辑卷
#格式化文件系统开始使用LV

pvcreate
pvscan 		扫描物理卷
pvdisplay 	物理卷参数
pvremoe 	删除物理卷组

vgcreate
vgscam
vgdisplay 
vgreduce 	收缩卷组
vgextend 	扩大卷组
vgremove	删除卷组

lvcreate
lvscan
lvs
lvdisplay
lvenxtend
lvreduce
lvextend
lvremove

 
```

### 创建流程

```shell

#查看存储信息
sky92@sky92-virtual-machine:~$ lsblk -s
NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
loop0    7:0    0     4K  1 loop /snap/bare/5
loop1    7:1    0    62M  1 loop /snap/core20/1611
loop2    7:2    0  63.2M  1 loop /snap/core20/1623
loop4    7:4    0 178.4M  1 loop /snap/firefox/1860
loop5    7:5    0 236.8M  1 loop /snap/firefox/1943
loop6    7:6    0 346.3M  1 loop /snap/gnome-3-38-2004/115
loop7    7:7    0  37.1M  1 loop /snap/hunspell-dictionaries-1-7-2004/2
loop8    7:8    0  91.7M  1 loop /snap/gtk-common-themes/1535
loop9    7:9    0    48M  1 loop /snap/snapd/16778
loop10   7:10   0    48M  1 loop /snap/snapd/17029
loop11   7:11   0 346.3M  1 loop /snap/gnome-3-38-2004/119
sda1     8:1    0     1M  0 part
└─sda    8:0    0    50G  0 disk
sda2     8:2    0   513M  0 part /boot/efi
└─sda    8:0    0    50G  0 disk
sda3     8:3    0  49.5G  0 part /var/snap/firefox/common/host-hunspell
│                                /
└─sda    8:0    0    50G  0 disk
sdb      8:16   0     2G  0 disk
sdc      8:32   0     2G  0 disk
sdd      8:48   0     2G  0 disk
sde      8:64   0     2G  0 disk
sr0     11:0    1  1024M  0 rom

#挑选俩个物理盘sdb sdc 实验
#创建物理卷
sky92@sky92-virtual-machine:~$ sudo pvcreate /dev/sdb /dev/sdc 
  Physical volume "/dev/sdb" successfully created.
  Physical volume "/dev/sdc" successfully created.
#此步骤可能之对裸盘有作用，如果这些硬盘先前分过区或使用过，我们需要wipefs首先擦除其分区表信息
sky92@sky92-virtual-machine:~$ sudo wipefs  --all --backup /dev/sdb /dev/sdc
/dev/sdb：8 个字节已擦除，位置偏移为 0x00000200 (gpt)： 45 46 49 20 50 41 52 54
/dev/sdb：8 个字节已擦除，位置偏移为 0x7ffffe00 (gpt)： 45 46 49 20 50 41 52 54
/dev/sdb：2 个字节已擦除，位置偏移为 0x000001fe (PMBR)： 55 aa
/dev/sdc：8 个字节已擦除，位置偏移为 0x00000200 (gpt)： 45 46 49 20 50 41 52 54
/dev/sdc：8 个字节已擦除，位置偏移为 0x7ffffe00 (gpt)： 45 46 49 20 50 41 52 54
/dev/sdc：2 个字节已擦除，位置偏移为 0x000001fe (PMBR)： 55 aa
/dev/sdc：将调用 ioctl 来重新读分区表：成功
/dev/sdb：将调用 ioctl 来重新读分区表：成功

sky92@sky92-virtual-machine:~$ sudo pvcreate -ff /dev/sdb /dev/sdc 
  Physical volume "/dev/sdb" successfully created.
  Physical volume "/dev/sdc" successfully created.

 
#=================================================================
#创建卷组
sky92@sky92-virtual-machine:~$ sudo vgcreate skyvg1  /dev/sdb /dev/sdc
  Volume group "skyvg1" successfully created
#=================================================================
#查看VG信息
sky92@sky92-virtual-machine:~$ sudo vgs
  VG     #PV #LV #SN Attr   VSize VFree
  skyvg1   2   0   0 wz--n- 3.99g 3.99g
 
sky92@sky92-virtual-machine:~$ sudo vgscan
  Found volume group "skyvg1" using metadata type lvm2
sky92@sky92-virtual-machine:~$ sudo vgdisplay
  --- Volume group ---
  VG Name               skyvg1
  System ID
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               3.99 GiB
  PE Size               4.00 MiB
  Total PE              1022
  Alloc PE / Size       0 / 0
  Free  PE / Size       1022 / 3.99 GiB
  VG UUID               thao0I-LYKL-wEG0-j80Z-7DyX-GqUm-IbXFdt

#=================================================================
#查看PV信息
sky92@sky92-virtual-machine:~$ sudo pvs
  PV         VG     Fmt  Attr PSize  PFree
  /dev/sdb   skyvg1 lvm2 a--  <2.00g <2.00g
  /dev/sdc   skyvg1 lvm2 a--  <2.00g <2.00g 
sky92@sky92-virtual-machine:~$ sudo pvscan
  PV /dev/sdb   VG skyvg1          lvm2 [<2.00 GiB / <2.00 GiB free]
  PV /dev/sdc   VG skyvg1          lvm2 [<2.00 GiB / <2.00 GiB free]
  Total: 2 [3.99 GiB] / in use: 2 [3.99 GiB] / in no VG: 0 [0   ]
sky92@sky92-virtual-machine:~$ sudo pvdisplay
  --- Physical volume ---
  PV Name               /dev/sdb
  VG Name               skyvg1
  PV Size               2.00 GiB / not usable 4.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              511
  Free PE               511
  Allocated PE          0
  PV UUID               br9Mh7-LWrS-Ckwl-evhD-UQPp-3DVe-IUorvd

  --- Physical volume ---
  PV Name               /dev/sdc
  VG Name               skyvg1
  PV Size               2.00 GiB / not usable 4.00 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              511
  Free PE               511
  Allocated PE          0
  PV UUID               dG0dOK-zC06-1NG9-TLXp-awFS-Fj1A-z9P4yA


#=================================================================
#扩容物理盘 加入一个sdd
sky92@sky92-virtual-machine:~$ sudo pvcreate /dev/sdd
  Physical volume "/dev/sdd" successfully created.
sky92@sky92-virtual-machine:~$ sudo vgextend  skyvg1 /dev/sdd
  Volume group "skyvg1" successfully extended

sky92@sky92-virtual-machine:~$ sudo vgdisplay  |grep 'VG'
  VG Name               skyvg1
  VG Access             read/write
  VG Status             resizable
  VG Size               <5.99 GiB
  VG UUID               thao0I-LYKL-wEG0-j80Z-7DyX-GqUm-IbXFdt

sky92@sky92-virtual-machine:~$ sudo vgdisplay  |grep 'VG\ Size'
  VG Size               <5.99 GiB

#缩减物理盘 
sky92@sky92-virtual-machine:~$ sudo vgreduce skyvg1 /dev/sdc
  Removed "/dev/sdc" from volume group "skyvg1" 
#删除物理卷
sky92@sky92-virtual-machine:~$ sudo pvremove /dev/sdc
  Labels on physical volume "/dev/sdc" successfully wiped. 
sky92@sky92-virtual-machine:~$ sudo pvs
  PV         VG     Fmt  Attr PSize  PFree
  /dev/sdb   skyvg1 lvm2 a--  <2.00g <2.00g
  /dev/sdd   skyvg1 lvm2 a--  <2.00g <2.00g

#=================================================================
#使用此时的卷组创建逻辑卷使用

sky92@sky92-virtual-machine:~$ sudo lvcreate -n lv1 -L +500M skyvg1
  Logical volume "lv1" created.
#检查
sky92@sky92-virtual-machine:~$ sudo lvs
  LV   VG     Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv1  skyvg1 -wi-a----- 500.00m
sky92@sky92-virtual-machine:~$ sudo lvdisplay
  --- Logical volume ---
  LV Path                /dev/skyvg1/lv1
  LV Name                lv1
  VG Name                skyvg1
  LV UUID                1u0cpw-CApB-ez8q-G1v1-8K8b-2q1U-bMIiQP
  LV Write Access        read/write
  LV Creation host, time sky92-virtual-machine, 2022-10-09 11:09:33 +0800
  LV Status              available
  # open                 0
  LV Size                500.00 MiB
  Current LE             125
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

#=================================================================
#格式化

sky92@sky92-virtual-machine:~$ sudo mkfs.ext4 /dev/skyvg1/lv1
mke2fs 1.46.5 (30-Dec-2021)
创建含有 128000 个块（每块 4k）和 128000 个 inode 的文件系统
文件系统 UUID：3c3a5feb-ab89-41bd-bfed-fb1c53313933
超级块的备份存储于下列块：
        32768, 98304

正在分配组表： 完成
正在写入 inode表： 完成
创建日志（4096 个块）： 完成
写入超级块和文件系统账户统计信息： 已完成


sky92@sky92-virtual-machine:~$ lsblk -f
NAME         FSTYPE      FSVER    LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINTS
loop0        squashfs    4.0                                                         0   100% /snap/bare/5
loop1        squashfs    4.0                                                         0   100% /snap/core20/1611
loop2        squashfs    4.0                                                         0   100% /snap/core20/1623
loop4        squashfs    4.0                                                         0   100% /snap/firefox/1860
loop5        squashfs    4.0                                                         0   100% /snap/firefox/1943
loop6        squashfs    4.0                                                         0   100% /snap/gnome-3-38-2004/115
loop7        squashfs    4.0                                                         0   100% /snap/hunspell-dictionaries-1-7-2004/2
loop8        squashfs    4.0                                                         0   100% /snap/gtk-common-themes/1535
loop9        squashfs    4.0                                                         0   100% /snap/snapd/16778
loop10       squashfs    4.0                                                         0   100% /snap/snapd/17029
loop11       squashfs    4.0                                                         0   100% /snap/gnome-3-38-2004/119
sda
├─sda1
├─sda2       vfat        FAT32          AFC7-CDA5                               506.7M     1% /boot/efi
└─sda3       ext4        1.0            b8849772-47bf-4414-847e-0e4d818c20c1     30.2G    32% /var/snap/firefox/common/host-hunspell
                                                                                              /
sdb          LVM2_member LVM2 001       br9Mh7-LWrS-Ckwl-evhD-UQPp-3DVe-IUorvd
└─skyvg1-lv1 ext4        1.0            3c3a5feb-ab89-41bd-bfed-fb1c53313933
sdc
sdd          LVM2_member LVM2 001       5YUbNo-vMW4-GqM6-fIA9-RV3o-Tl5W-dxc9wL
sde
sr0

#=================================================================
#挂载使用
sky92@sky92-virtual-machine:~$ cd /mnt/ 
sky92@sky92-virtual-machine:/mnt$ sudo mkdir vg1
sky92@sky92-virtual-machine:/mnt$ sudo mount /dev/skyvg1/lv1 /mnt/vg1/ 
sky92@sky92-virtual-machine:/mnt$ sudo df -hT
文件系统               类型     容量  已用  可用 已用% 挂载点
tmpfs                  tmpfs    389M  2.1M  387M    1% /run
/dev/sda3              ext4      49G   16G   31G   35% /
tmpfs                  tmpfs    1.9G     0  1.9G    0% /dev/shm
tmpfs                  tmpfs    5.0M  4.0K  5.0M    1% /run/lock
/dev/sda2              vfat     512M  5.3M  507M    2% /boot/efi
tmpfs                  tmpfs    389M   48K  389M    1% /run/user/115
overlay                overlay   49G   16G   31G   35% /var/lib/docker/overlay2/6bc5f84601a943199120571e57d32e11417dffa40ad7bf69156b9bd7d0f8a951/merged
overlay                overlay   49G   16G   31G   35% /var/lib/docker/overlay2/1d5926d9f2d99080a7a5979498171c2cd651d81ebfecbbc2035d7d79da76b328/merged
overlay                overlay   49G   16G   31G   35% /var/lib/docker/overlay2/104b3d578cad3c06718aae6172d5441da11144589f25666fce3a8bfdb0cce4d2/merged
overlay                overlay   49G   16G   31G   35% /var/lib/docker/overlay2/9584e2cd52ffecc3c180ffbee90e9182885b2c6d0804b9cf6cf8eef93156b596/merged
tmpfs                  tmpfs    389M   44K  389M    1% /run/user/1000
/dev/mapper/skyvg1-lv1 ext4     452M   24K  417M    1% /mnt/vg1

#如果要开机挂载 则编辑/etc/fstab 首先获得UUID
sky92@sky92-virtual-machine:/mnt/vg1$ lsblk -f
NAME         FSTYPE      FSVER    LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINTS
loop0        squashfs    4.0                                                         0   100% /snap/bare/5
loop1        squashfs    4.0                                                         0   100% /snap/core20/1611
loop2        squashfs    4.0                                                         0   100% /snap/core20/1623
loop4        squashfs    4.0                                                         0   100% /snap/firefox/1860
loop5        squashfs    4.0                                                         0   100% /snap/firefox/1943
loop6        squashfs    4.0                                                         0   100% /snap/gnome-3-38-2004/115
loop7        squashfs    4.0                                                         0   100% /snap/hunspell-dictionaries-1-7-2004/2
loop8        squashfs    4.0                                                         0   100% /snap/gtk-common-themes/1535
loop9        squashfs    4.0                                                         0   100% /snap/snapd/16778
loop10       squashfs    4.0                                                         0   100% /snap/snapd/17029
loop11       squashfs    4.0                                                         0   100% /snap/gnome-3-38-2004/119
sda
├─sda1
├─sda2       vfat        FAT32          AFC7-CDA5                               506.7M     1% /boot/efi
└─sda3       ext4        1.0            b8849772-47bf-4414-847e-0e4d818c20c1     30.2G    32% /var/snap/firefox/common/host-hunspell
                                                                                              /
sdb          LVM2_member LVM2 001       br9Mh7-LWrS-Ckwl-evhD-UQPp-3DVe-IUorvd
└─skyvg1-lv1 ext4        1.0            3c3a5feb-ab89-41bd-bfed-fb1c53313933    416.4M     0% /mnt/vg1
sdc
sdd          LVM2_member LVM2 001       5YUbNo-vMW4-GqM6-fIA9-RV3o-Tl5W-dxc9wL
sde
sr0
#得到UUID=3c3a5feb-ab89-41bd-bfed-fb1c53313933

sky92@sky92-virtual-machine:~$ cd ~
sky92@sky92-virtual-machine:~$ sudo umount /mnt/vg1

#=================================================================
#lv扩容 只要卷组内容量够用 就能扩容
sky92@sky92-virtual-machine:~$ sudo lvextend  -L +2G /dev/skyvg1/lv1
  Size of logical volume skyvg1/lv1 changed from 500.00 MiB (125 extents) to <2.49 GiB (637 extents).
  Logical volume skyvg1/lv1 successfully resized.

sky92@sky92-virtual-machine:~$ sudo lvdisplay
  --- Logical volume ---
  LV Path                /dev/skyvg1/lv1
  LV Name                lv1
  VG Name                skyvg1
  LV UUID                1u0cpw-CApB-ez8q-G1v1-8K8b-2q1U-bMIiQP
  LV Write Access        read/write
  LV Creation host, time sky92-virtual-machine, 2022-10-09 11:09:33 +0800
  LV Status              available
  # open                 0
  LV Size                <2.49 GiB
  Current LE             637
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

#挂载后需要做调节文件大小才能使用
#挂载
sky92@sky92-virtual-machine:~$ sudo mount /dev/skyvg1/lv1 /mnt/vg1/
#查看大小
sky92@sky92-virtual-machine:~$ sudo df -hT |grep skyvg1 
/dev/mapper/skyvg1-lv1 ext4     452M  600K  417M    1% /mnt/vg1
#开始调整
sky92@sky92-virtual-machine:~$ sudo resize2fs -f /dev/skyvg1/lv1
resize2fs 1.46.5 (30-Dec-2021)
/dev/skyvg1/lv1 上的文件系统已被挂载于 /mnt/vg1；需要进行在线调整大小
old_desc_blocks = 1, new_desc_blocks = 1
/dev/skyvg1/lv1 上的文件系统大小已经调整为 652288 个块（每块 4k）。
#查看大小
sky92@sky92-virtual-machine:~$ sudo df -hT |grep skyvg1
/dev/mapper/skyvg1-lv1 ext4     2.4G  600K  2.3G    1% /mnt/vg1
#这里的lv1是ext4格式 使用的是 resize2fs 调整如果是xfs格式 则需要使用 xfs_growfs

#=================================================================
#删除lvm
#卸载
sky92@sky92-virtual-machine:~$ sudo umount /mnt/vg1
#删除逻辑卷
sky92@sky92-virtual-machine:~$ sudo lvremove /dev/skyvg1/lv1
Do you really want to remove and DISCARD active logical volume skyvg1/lv1? [y/n]: y
  Logical volume "lv1" successfully removed
#删除卷组
sky92@sky92-virtual-machine:~$ sudo vgremove skyvg1
  Volume group "skyvg1" successfully removed

#查看物理卷
sky92@sky92-virtual-machine:~$ sudo pvs
  PV         VG Fmt  Attr PSize PFree
  /dev/sdb      lvm2 ---  2.00g 2.00g
  /dev/sdd      lvm2 ---  2.00g 2.00g

#删除物理卷
sky92@sky92-virtual-machine:~$ sudo pvremove /dev/sdb /dev/sdd
  Labels on physical volume "/dev/sdb" successfully wiped.
  Labels on physical volume "/dev/sdd" successfully wiped.

#检查所有
sky92@sky92-virtual-machine:~$ sudo lvs
sky92@sky92-virtual-machine:~$ sudo vgs
sky92@sky92-virtual-machine:~$ sudo pvs


 


```



















