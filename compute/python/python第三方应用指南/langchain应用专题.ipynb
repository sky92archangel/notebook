{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义使用的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sky92\\AppData\\Local\\Temp\\ipykernel_8432\\3643654744.py:4: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# 创建 Ollama 实例\n",
    "# 确保 Ollama 服务正在运行，并且模型已经拉取到本地\n",
    "llm = Ollama(\n",
    "    base_url='http://127.0.0.1:11434',  # 本地 Ollama 服务地址 其实默认就是这个地址\n",
    "    model='qwen2:7b',  # 你要使用的模型名称\n",
    "    timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用本地ollama 保留链式处理 完整例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    " \n",
    "# 创建 PromptTemplate 模板定义\n",
    "# my_prompt = PromptTemplate.from_template(\"\"\"回答这个问题:{input}\"\"\")\n",
    "\n",
    "my_prompt = PromptTemplate.from_template([\n",
    "    (\"system\",\"你是世界级技术专家\"),\n",
    "    (\"user\",\"{input}\"),\n",
    "])\n",
    "\n",
    "# 创建 Ollama 实例\n",
    "# 确保 Ollama 服务正在运行，并且模型已经拉取到本地\n",
    "ollama = Ollama(\n",
    "    base_url='http://127.0.0.1:11434',  # 本地 Ollama 服务地址\n",
    "    model='qwen2:7b',  # 你要使用的模型名称\n",
    "    timeout=60\n",
    ")\n",
    "\n",
    "# 链式动作可调用api 调用数据库  爬虫 结果储存 等动作\n",
    "# 创建链式调用  模板定义->大模型处理->输出内容解析   \n",
    "chain = my_prompt | ollama | StrOutputParser()\n",
    "\n",
    "# 调用链式调用\n",
    "output = chain.invoke({\"input\": \"你是谁？20字以内回答\"})\n",
    "\n",
    "# 打印输出\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提示词模板工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个人工智能，名字叫bob', additional_kwargs={}, response_metadata={}), HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='我很好，谢谢', additional_kwargs={}, response_metadata={}), HumanMessage(content='你好', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# 直接编码\n",
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "chat_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"你是一个人工智能，名字叫{name}\"),\n",
    "        (\"human\",\"你好\"),\n",
    "        (\"ai\",\"我很好，谢谢\"),\n",
    "        (\"human\",\"{user_input}\"),\n",
    "    ]\n",
    ") \n",
    "\n",
    "messages = chat_template.format_messages(name=\"bob\",user_input=\"你好\") \n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个职业秘书{name}，可以润色内容', additional_kwargs={}, response_metadata={}), HumanMessage(content='你好，你是谁', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# 结构化编码\n",
    "from langchain_core.prompts import ChatPromptTemplate ,HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage \n",
    "\n",
    "chat_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "        content=(\n",
    "            \"你是一个职业秘书{name}，可以润色内容\"\n",
    "        )),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\")      \n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(name=\"jack\",text=\"你好，你是谁\") \n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='你是一个秘书', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}), HumanMessage(content='hello!', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Placeholder 使用\n",
    "from langchain_core.prompts import ChatPromptTemplate ,MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage ,HumanMessage\n",
    "\n",
    "chat_template= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"你是一个秘书\"),\n",
    "        MessagesPlaceholder(\"msgs\")      \n",
    "    ]\n",
    ")\n",
    "\n",
    "# 修复后的代码\n",
    "messages = chat_template.invoke({\n",
    "    \"msgs\": [HumanMessage(content=\"hi!\"), HumanMessage(content=\"hello!\")]\n",
    "})\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提示词最佳提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题：谁的寿命更长，穆罕默德·阿里还是艾伦·图灵?\\n\n",
      "                这里需要跟进问题吗:是的。\n",
      "                跟进:穆罕默德·阿里去世时多大?\n",
      "                中间答案:穆罕默德·阿里去世时74岁\n",
      "                跟进:艾伦·图灵去世时多大?\n",
      "                中间答案:艾伦·图灵去世时41岁\n",
      "                所以最终答案是:穆罕默德·阿里\n",
      "\n",
      "问题：craigslist的创始人是什么时候出生的?\\n\n",
      "                这里需要跟进问题吗:是的。\n",
      "                跟进:craigslist的创始人是谁?\n",
      "                中间答案:craigslist由Craig Newmark创立。\n",
      "                跟进:Craig Newmark是什么时候出生的?\n",
      "                中间答案:Craig Newmark于1952年12月6日出生。\n",
      "                所以最终答案是:1952年12月6日\n",
      "\n",
      "问题：乔治·华盛顿的祖父母中的母亲是谁?\\n\n",
      "                这里需要跟进问题吗:是的。\n",
      "                跟进:乔治1华盛顿的母亲是谁?\n",
      "                中间答案:乔治·华盛顿的母亲是Mary Ball Washington。\n",
      "                跟进:Mary Ball Washington的父亲是谁?\n",
      "                中间答案:Mary Ball Washington的父亲是Joseph Ball。\n",
      "                所以最终答案是:Joseph Ball\n",
      "                \n",
      "\n",
      "问题：大白鲨和皇家赌场的导演是同一个国家的吗?\\n\n",
      "                这里需要跟进问题吗:是的。\n",
      "                跟进:《大白鲨》的导演是谁?\n",
      "                中间答案:《大白鲨》的导演是Steven Spielberg.\n",
      "                跟进:Steven Spielberg来自哪里?\n",
      "                中间答案:美国。\n",
      "                跟进:《皇家赌场》的导演是谁?\n",
      "                中间答案:《皇家赌场》的导演是Martin Campbell.\n",
      "                跟进:Martin Campbell来自哪里?\n",
      "                中间答案:新西兰。\n",
      "                所以最终答案是:不是\n",
      "\n",
      "问题:《大白鲨》的导演是哪个国家的？\n",
      "问题：谁的寿命更长，穆罕默德·阿里还是艾伦·图灵?\\n\n",
      "                这里需要跟进问题吗:是的。\n",
      "                跟进:穆罕默德·阿里去世时多大?\n",
      "                中间答案:穆罕默德·阿里去世时74岁\n",
      "                跟进:艾伦·图灵去世时多大?\n",
      "                中间答案:艾伦·图灵去世时41岁\n",
      "                所以最终答案是:穆罕默德·阿里\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "examples= [\n",
    "    {\n",
    "        \"question\":\"谁的寿命更长，穆罕默德·阿里还是艾伦·图灵?\",\n",
    "        \"answer\":'''\n",
    "                这里需要跟进问题吗:是的。\n",
    "                跟进:穆罕默德·阿里去世时多大?\n",
    "                中间答案:穆罕默德·阿里去世时74岁\n",
    "                跟进:艾伦·图灵去世时多大?\n",
    "                中间答案:艾伦·图灵去世时41岁\n",
    "                所以最终答案是:穆罕默德·阿里'''\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"craigslist的创始人是什么时候出生的?\",\n",
    "        \"answer\":'''\n",
    "                这里需要跟进问题吗:是的。\n",
    "                跟进:craigslist的创始人是谁?\n",
    "                中间答案:craigslist由Craig Newmark创立。\n",
    "                跟进:Craig Newmark是什么时候出生的?\n",
    "                中间答案:Craig Newmark于1952年12月6日出生。\n",
    "                所以最终答案是:1952年12月6日'''\n",
    "    },\n",
    "    {\n",
    "        \"question\":\"乔治·华盛顿的祖父母中的母亲是谁?\",\n",
    "        \"answer\" : \n",
    "                '''\n",
    "                这里需要跟进问题吗:是的。\n",
    "                跟进:乔治1华盛顿的母亲是谁?\n",
    "                中间答案:乔治·华盛顿的母亲是Mary Ball Washington。\n",
    "                跟进:Mary Ball Washington的父亲是谁?\n",
    "                中间答案:Mary Ball Washington的父亲是Joseph Ball。\n",
    "                所以最终答案是:Joseph Ball\n",
    "                '''\n",
    "        },\n",
    "    {\n",
    "        \"question\":\"大白鲨和皇家赌场的导演是同一个国家的吗?\",\n",
    "        \"answer\":'''\n",
    "                这里需要跟进问题吗:是的。\n",
    "                跟进:《大白鲨》的导演是谁?\n",
    "                中间答案:《大白鲨》的导演是Steven Spielberg.\n",
    "                跟进:Steven Spielberg来自哪里?\n",
    "                中间答案:美国。\n",
    "                跟进:《皇家赌场》的导演是谁?\n",
    "                中间答案:《皇家赌场》的导演是Martin Campbell.\n",
    "                跟进:Martin Campbell来自哪里?\n",
    "                中间答案:新西兰。\n",
    "                所以最终答案是:不是'''\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "#提示词解析模板\n",
    "example_prompt = PromptTemplate(input_variables=[\"quetion\",\"anwser\"],template=\"问题：{question}\\\\n{answer}\")\n",
    " \n",
    "prompt=FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"问题:{input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "print(prompt.format (input=\"《大白鲨》的导演是哪个国家的？\"))\n",
    " \n",
    "#只传入顶一个实例样本\n",
    "print(example_prompt.format(**examples[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例选择器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sky92\\AppData\\Local\\Temp\\ipykernel_1104\\812084880.py:57: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "#实例选择器\n",
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings \n",
    " \n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3:latest\") \n",
    "#语意相似性\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(#这是可供选择的示例列表。\n",
    "    examples,\n",
    "    #这是用于生成嵌入的嵌犬类，该嵌入用于衡量语义相似性。\n",
    "    embeddings ,\n",
    "    #这是用于存储嵌入和执行相似性搜索的Vectorstore类\n",
    "    Chroma,\n",
    "    #这是要生成的示例数\n",
    "    k=1\n",
    ")\n",
    "\n",
    "#选择与输入最相似的示例。\n",
    "question =\"乔治·华盛顿的父亲是谁?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"最相似的示例:{question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\\\n\")\n",
    "    for k,v in example.items():\n",
    "        print(f\"{k}:{v}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 链式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "# 创建 Ollama 实例\n",
    "# 确保 Ollama 服务正在运行，并且模型已经拉取到本地\n",
    "llm = Ollama(\n",
    "    base_url='http://127.0.0.1:11434',  # 本地 Ollama 服务地址 其实默认就是这个地址\n",
    "    # model='qwen2:7b',  # 你要使用的模型名称\n",
    "    model='deepseek-r1:8b',  # 你要使用的模型名称\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks =[]\n",
    "for chunk in llm.stream(\"天空是什么颜色的？,二十字以内！\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk ,end=\"|\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#astream chain.py  异步流式调用\n",
    "# import asyncio\n",
    "\n",
    "# #jupyter专用嵌套异步\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"给我讲一个关于{topic}的笑话,快速判断，无需显示think，二十字以内！\")\n",
    "parser =StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\":\"鹦鹉\"}):\n",
    "        print(chunk, end=\"\",flush=True)\n",
    "\n",
    "# async def async_stream():\n",
    "#     async for chunk in chain.astream({\"topic\":\"鹦鹉\"}):\n",
    "#         print(chunk, end=\"\",flush=True)\n",
    "\n",
    "# asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标准json输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    " \n",
    "parser =JsonOutputParser()\n",
    "chain = prompt|llm| parser\n",
    " \n",
    " \n",
    "\n",
    "# async for chunk in chain.astream(\n",
    "#     \"以JSON 格式输出法国、西班牙和日本的国家及其人口列表。\"\n",
    "#     '使用一个带有“countries”外部键的字典，其中包含国家列表。'\n",
    "#     \"每个国家都应该有键`name`和`population`\"\n",
    "#     ): \n",
    "#         print(chunk, end=\"\",flush=True)\n",
    "\n",
    "\n",
    "for chunk in llm.stream(\n",
    "    \"以JSON 格式输出法国、西班牙和日本的国家及其人口列表。\"\n",
    "    '使用一个带有“countries”外部键的字典，其中包含国家列表。'\n",
    "    \"每个国家都应该有键`name`和`population`\"\n",
    "):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk ,end=\"\",flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#示例:tools retriever.py\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "from  langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(r\"https://zh.wikipedia.org/wiki/%E7%8C%AB\")\n",
    "print(loader,\"\\n\")\n",
    "docs = loader.load()\n",
    "documents =RecursiveCharacterTextSplitter(\n",
    "    #chunksize 参数在 RecursivecharacterTextsplitter 中用于指定每个文档块的最大字符数。它的作\n",
    "    #chunk overlap 参数用于指定每个文档块之间的重着字符数。这意味着，当文档被拆分成较小的块时，每\n",
    "    # 第一个块包含字符 1 到 1088。第二个块包含字符881到 1888。\n",
    "    # 第三个块包含字符 1601 到 2680。\n",
    "    chunk size=1000,chunk overlap=200\n",
    "    ).split documents(docs)\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "\n",
    "vector =FAISS.from_documents(documents,embeddings())\n",
    "retriever =vector.as_retriever()\n",
    "\n",
    "print(retriever.invoke(\"猫的特征\")[0])\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever,\"wiki_search\",\"搜索维基百科\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENT调用代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到提示词\n",
    "from langchain import hub\n",
    "#获取要使用的提示-您可以修改这个!  langchain官方的提示词仓库\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "print(prompt.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立agent\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "# 各工具集\n",
    "tools=[search,retriever_tool]\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始执行agent\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent,tools=tools,verbose=True)\n",
    "print(agent_executor.invoke({\"input\":\"你好\"}))\n",
    "print(agent_executor.invoke({\"input\":\"猫的特征？\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
