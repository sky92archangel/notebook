# 导论 通讯简介

[TOC]

## 通信

通讯就是传输信息 

### 信使三姐妹

信息 消息 信号 ；信息必然以某种形式存在；

消息是信息的载体，也是信息的表现，一般为声音、图像、文字、温度、符号等，有连续和离散之分；

信息：消息中的有效内容；同样的信息可使用不同形式的消息来表达；

信号：是消息的光、电、波的表达形式；一般有模拟信号和数字信号区分；一般使用电信号或者光信号形式来传递；

消息一般通过传感器转换为电信号；

模拟信号取值连续，数字型号取值离散；

**通讯就是利用光电或波动的信号传输消息中包含的信息；**

### 通讯历程

略

### 通讯意义

通讯的目的在于传输信息；

信息需要借助于物质和能量才能产生、传输、存储、处理、感知；

信息的价值在于传播；

应用于经济 文化 军事 教育 政治 等领域；

## 通讯系统的组成

### 模拟通讯模型

AM 调幅为例  

【声音】 --- 麦克风 --- 发射机发射 --- 【AM无线电波】 --- 收音机接受 --- 扬声器  --- 【声音】

得到一般模型

**信源 -- 发送设备 -- 信道 -- 接收设备 -- 信宿** 

source  --  transmitter  --  receiver  --  destination

source：消息 -- 电信号 ，如话筒

transmitter：电信号的模式转变  编码 调制

receiver：电信号的模式转变 解调 译码 

destination：电信号 -- 消息  ，如喇叭 听筒

#### 信号模式

##### 基带传输

消息信号为基带信号，基础频谱位于0附近；

话音信号基本频率为  300-3400Hz

一般电视信号频率为 0-6 MHz 高清则是 0- 20MHz  

像是基带信号 一般可以在实体线路中传输，如音频、视频线；这就是基带传输；

##### 带通传输

如果需要无线电传输，则需要提高频率范围，达到带通信号范围；这就是调制器的工作；其逆过程就是解调器的工作；

这类就叫 带通传输 调制传输 频带传输

### 数字通讯模型

**信源 -- 信源编码  --  加密 -- 信道编码  --  数字解调  --  信道  --  数字解调  --  信道译码  --  解密 -- 信源译码 -- 信宿** 

信源编码： 数模转换，压缩，提高密度

信道编码： 冗余 提供可靠 

调制：把信息载波 

解调：从已调信号中卸载信息

基本涵盖，编码 译码 调制 解调 同步；

数字通讯 抗噪能力强 误差可控 易于加密；

## 通讯分类和方式

按信道特征：模拟、数字

传输媒介：有线、无线

传输方式：基带传输、带通传输

业务类型：电话、图像

波段分类：长中短波 微波 紫外 可见光    $\lambda=\frac cf$

复用方式：频分、时分、码分

#### 通信方式：

单工通信：发送端只管理发送，接收端只管理接受，典型就是收音机

半双工通信：双向不同时，每一段同时只能发送或接收，典型就是对讲机

双工通信：同时双向，两方可以同时发送和接收信息；

#### 传输方式：

并行传输：合适近距离，同时发送一组信号；

串行传输：适合远距离，数据排队传输；

## 信息度量

### 信息量

信息量：就是不确定性的多少，也就是确定性的多少；概率越小、信息量越大

信息量I是概率P的函数  ，独立事件的概率同时发生需相乘计算  ，而独立事件的所含信息量则需要加法计算，所以其可加 ；
$$
I=f[P(x)] \\ 
P \to 1 , I \to 0 \\ 
P \to 0 , I \to \infin \\
$$
那么我们可以构建一个基本函数来描述这个关系 该公式用于度量点单个情况不确定度
$$
I=\log_a \frac1{P(x)}=\log_a P(x)  \\
I_{(nat)}=\log_a \frac1{P(x)}=\log_e P(x)  \\
I_{(hartley)}=\log_a \frac1{P(x)}=\log_{10} P(x)  \\
I_{(bit)}=\log_a \frac1{P(x)}=\log_2 P(x)  \\
$$
当该信息量计算底数为2时就是香农提出的，其信息量纲为bit，此时离散消息的（自）信息量更加好计算

于是在计算机中 由于信号只有 0 和 1 两个可能性  这是二进制信源 ， 在该情况下：
那么就得到 两个概率 $P(0)=P(1)=\frac12$ 
也就是信息量为 $I_0=I_1=\log_22=1b$

如果我们有一个四进制的信源 发送为 0 1 2 3 
那么信号的发送就有四个概率  $P(0)=P(1)=P(2)=P(3)=\frac14$
于是信息量就是 $I_0=I_1=I_2=I_3=\log_2 4=2b$

那么总结为 二进制每个码源1bit信息，四进制每个码源为2bit
推广后就是 M进制的每个码源为kbit，$\log_2M=k , M = 2^k$ 

再者，二进制的信源（0，1），每个符号独立出现，若 0 的出现概率为 1/3 ，那么每个符号信息量又如何？
已知 $P(0)=\frac13 \to P(1)=\frac23 $
$I_0 = \log_2 3=1.584 bit,I_1=\log_2\frac32=0.585bit $

由此可见 ，概率影响信息量，概率小、难确定、信息量大，符号的不确定性无法反映整个信源不确定性

### 信源熵  

为1948年香农提出

如果具有M个符号的离散信息源 则  $\sum\limits^M_i P(x_i) =1$

则 信源中每个符号的平均信息量 即 熵
$$
H=E[I(x_i)]=E[\log_2\frac1{P(x_i)}]=\sum\limits^M_{i=1}[ P(x_i)\log_2\frac1{P(x_i)}] \ \ (b/symbol)
$$
熵的意义就是信源的平均不确定度；
所以，如果熵越大，不确定性越大，那么将其搞清楚的信息量就越大；
信息的基本作用就是消除人们对事物的不确定，将思维贴合与固着在现实上；

那么如果我们具有四进制信息源 每个概率分别为  $P(0)=\frac38,P(1)=\frac14,P(2)=\frac14,P(3)=\frac18$ 要求信源熵

我们可直接使用公式 

$I(0)=\log_2\frac1{P(0)}=\log_2\frac83 =1.416bit,H(0)=P(0)I(0)=\frac38\log_2\frac83=0.531$

$ I(1)=\log_2\frac1{P(1)}=\log_24 = 2bit,H(1)=P(1)I(1)=0.5$

$ I(2)=\log_2\frac1{P(2)}=\log_24 = 2bit,H(2)=P(2)I(2)=0.5$

$ I(3)=\log_2\frac1{P(3)}=\log_28 = 3bit ,H(3)=P(3)I(3)=\frac38=0.375$

$H(0)+H(1)+H(2)+H(3)=H= 1.906 bit/sym$

等概率时熵最大

我们可以进行验证

我们使用上述不等概信源 发送长度57的消息 2010201302130012032101003210100231020020131203210012021
直接数得消息里  23个0， 14个1， 13个2 ，  7个3    $I=23I_0+14I_1+13I_2+7I_3 = 108bit$  如此计算较为复杂
利用熵得概念来计算：$I=57\times H = 57\times 1.906=108.64bit$



## 性能指标

有效和可靠矛盾

模型通讯使用传输带宽衡量有效性，信噪比衡量可靠性；
数字通信使用传输速率衡量有效性，差错概率衡量可靠性；

### 有效

数字通信系统有效性指标

码元传输速率$R_B$（波特率、传码率、baud）：每秒传输码元符号个数，单位 
如果码元持续时间$T_B$，则码元长度为 $R_B=\frac1{T_B}$
$T_B=1ms \to R_B=1000Baud , 一秒内1000个码元$

信息传输速率$R_b$  (比特率、传信率、bit/s、bps)：

定义每秒传输的比特数（信息量），每个码元带有信息量，将这些集合起来

等概率的时候 比特率和波特率由信息熵来联系
$$
R_b=R_B\cdot H \\
R_b=R_B\cdot \log_2 M \\
$$
频带利用率 单位带宽内的传输速率：
$$
\eta=\frac{R_B}B,(Baud/Hz)\\
\eta_b=\frac{R_b}B,(bps/Hz)\\
 \eta_b=\eta\cdot\log_2M
$$

### 可靠性

误码率
$$
P_e=\frac{错误码元数}{传输总码元}=\frac{N_e}{N}\\
$$
误信率
$$
P_b=\frac{错误比特数}{传输总比特}=\frac{I_e}{I}\\
$$
二进制时  $P_b=P_e$



